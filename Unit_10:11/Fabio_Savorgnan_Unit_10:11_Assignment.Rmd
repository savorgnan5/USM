---
title: "Fabio Savorgnan Unit 10/11 Assignment"
author: "Fabio"
date: "7/15/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#Dependencies

###This are the libraries that we will be using in this project.

```{r, message=F, warning = F, echo=TRUE}
library(dplyr)
library(ggplot2)
library(tidyr)
library(reshape2)
library(tidyverse)
library(stringr)
library(caret)
library(SDMTools)
```


### We are loading the dataset from the beers and the dataset from the Breweries.

```{r, message=F, warning = F, echo=TRUE, echo=TRUE}
url<-"https://raw.githubusercontent.com/BivinSadler/MSDS-6306-Doing-Data-Science/master/Unit%207/Beers.csv"
beers <- read.csv(url(url), sep = ",", header = TRUE)
head(beers) 
url<-"https://raw.githubusercontent.com/BivinSadler/MSDS-6306-Doing-Data-Science/master/Unit%207/Breweries.csv"
brew <- read.csv(url(url), sep = ",", header = TRUE)
head(brew) 
```

### We clean the data and combined the two dataset.
```{r, message=F, warning = F, echo=TRUE, echo=TRUE}
# changing from factor to char
brew$State <- as.character(brew$State)
# changing to int
brew$Brew_ID <- as.integer(brew$Brew_ID)
# changing to char
brew$Name <-  as.character(brew$Name)
# removing trailing whitespace with str_trim
str_trim(brew$State, side = "left") -> brew$State
# Cleaning to combine bataset
beers$Brew_ID<-beers$Brewery_id
beers$Brewery_id<-NULL

```


```{r, message=F, warning = F, echo=TRUE, echo=TRUE}
combined<- merge(brew, beers, by = "Brew_ID")
# renaming Name.x to Brewery_Name
colnames(combined)[2] <- "Brewery_Name"
# renaming Name.y to Beer_Name
colnames(combined)[5] <- "Beer_Name"
head(combined)
```


### Create one new dataset for Colorado and Texas. And create a traning set and a test set only for Texas.

```{r, message=F, warning = F, echo=TRUE, echo=TRUE}
na.omit(combined) %>% filter(State %in% c("TX","CO"))%>% arrange(IBU) ->beerCOTX
head(beerCOTX)
```



```{r, message=F, warning = F, echo=TRUE, echo=TRUE}
na.omit(combined) %>% filter(State %in% c("TX"))%>% arrange(IBU) ->beerTX
head(beerTX)
## 60% and 40% of the sample size
smp_size <- floor(0.60 * nrow(beerTX))
## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(beerTX)), size = smp_size)
trainTX <- beerTX[train_ind, ]
head(trainTX)
testTX <- beerTX[-train_ind, ]
head(testTX)

```

### Summary of the train and test 
```{r, message=F, warning = F, echo=TRUE, echo=TRUE}
summary(trainTX)

summary(testTX)

```


### Model 1 KNN

```{r, message=F, echo=TRUE}
trainTX$ABV<- as.matrix(trainTX$ABV)
fit<- knnreg(x = trainTX$ABV, y = trainTX$IBU, K = 3)
fit
predictTX1<- predict(fit, testTX$ABV)
predictTX1
```

### Model 2 KNN

```{r, message=F, warning = F, echo=TRUE, echo=TRUE}
trainTX$ABV<- as.matrix(trainTX$ABV)
fit<- knnreg(x = trainTX$ABV, y = trainTX$IBU, K = 5)
fit
predictTX2<- predict(fit, testTX$ABV)
predictTX2
```

### ASE for model 1 and 2. This second model are the same because the ASE1 and ASE2 are the same number. ASE is the difference between the predicted value and the value to be predicted from.
```{r, message=F, warning = F, echo=TRUE, echo=TRUE}
ASE1 <-mean((predictTX1-trainTX$ABV)^2)
ASE1
ASE2 <-mean((predictTX1-trainTX$ABV)^2)
ASE2
```

### The model from the last week is better than this model, plese see below, the value from the ASE is lower in ASEC which is the linear regrsion model from last week in comparison to the ASE1 and ASE2 which are the KNN model from this week.

```{r, message=F, warning = F, echo=TRUE, echo=TRUE}
texas <- lm(trainTX$ABV ~trainTX$IBU) 
summary(texas)
# prediction 1
predictl <- predict.lm(texas, testTX)
head(predictl)
ASEc <- mean((predictl-testTX$ABV)^2)
ASEc
```

### Predict ABV in based of the following IBU 150, 170, 190. The issue that I see when we extrapolate is that the prediction may not be applicable for what we want to apply.Extrapolation is subject to greater uncertainty and a higher risk of producing meaningless results. Extrapolation may also mean extension of a method, assuming similar methods will be applicable.


```{r, message=F, warning = F, echo=TRUE, echo=TRUE}
test<-c(150, 170, 190)
fit<- knnreg(x = trainTX$ABV, y = trainTX$IBU  , K = 3)
fit
predictn<- predict(fit, test)
predictn
```


### New dataset with Style of the beer in Texas

```{r, message=F, warning = F, echo=TRUE, echo=TRUE}
na.omit(combined) %>% filter(State %in% c("TX"))%>% arrange(IBU) ->beerSTX
beerSTX  %>% filter(Style %in% c("American IPA", "American Pale Ale (APA)"))%>% arrange(IBU)-> beerSTX 
beerSTX 
```

### Divide this filtered data set into a training and test set (60/40, training / test split).  

```{r, message=F, warning = F, echo=TRUE, echo=TRUE}
## 60% and 40% of the sample size
smp_size <- floor(0.60 * nrow(beerSTX))
## set the seed to make your partition reproducible
set.seed(123)
train_ind <- sample(seq_len(nrow(beerSTX)), size = smp_size)
trainSTX <- beerSTX[train_ind, ]
head(trainSTX)
testSTX <- beerSTX[-train_ind, ]
head(testSTX)

```


### Model 1 predicting style in base of ABV and IBU

```{r, message=F, warning = F, echo=TRUE, echo=TRUE}
trainSTX$Style  <- as.character(trainSTX$Style)
testSTX$Style <- as.character(testSTX$Style)
results = class::knn(trainSTX[,c(7,8)], testSTX[,c(7,8)], trainSTX$Style, k = 3)
results
conf<- table(testSTX$Style, results)
caret::confusionMatrix(conf)
```


### Model 2 predicting style in base of ABV and IBU.

### This two model are very similar. There is not difference between the two, I will take the first model because it is simpler.
```{r, message=F, warning = F, echo=TRUE, echo=TRUE}
trainSTX$Style  <- as.character(trainSTX$Style)
testSTX$Style <- as.character(testSTX$Style)
results = class::knn(trainSTX[,c(7,8)], testSTX[,c(7,8)], trainSTX$Style, k = 5)
results
conf<- table(testSTX$Style, results)
caret::confusionMatrix(conf)
```

### Unit 11

```{r, message=F, warning = F, echo=TRUE, echo=TRUE}

library(tm) #text mining library provides the stopwords() function
library(tidyr)
library(plyr)
library(jsonlite)
library(dplyr)
library(tidyverse)
```


### NYT API and get data

```{r, message=F, warning = F, echo=TRUE, echo=TRUE}
NYTIMES_KEY = "ULAGIigdFMCFsmJgB5cxcwKmCjnISU6W" #Your Key Here â€¦ get from NTY API website
# Let's set some parameters
term <- "Central+Park+Jogger" # Need to use + to string together separate words
begin_date <- "19890419"
end_date <- "19990419"
baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term,
                  "&begin_date=",begin_date,"&end_date=",end_date,
                  "&facet_filter=true&api-key=",NYTIMES_KEY, sep="")
baseurl

```

### Put the data together

```{r, message=F, warning = F, echo=TRUE, echo=TRUE}
initialQuery <- jsonlite::fromJSON(baseurl)
maxPages <- round((initialQuery$response$meta$hits[1] / 10)-1)
maxPages
pages <- list()
for(i in 0:maxPages){
  nytSearch <- jsonlite::fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame() 
  message("Retrieving page ", i)
  pages[[i+1]] <- nytSearch 
  Sys.sleep(7) 
}
allNYTSearch <- rbind_pages(pages)

```

### Visualize coverage by section
```{r, message=F, warning = F, echo=TRUE, echo=TRUE}
allNYTSearch %>% 
  group_by(response.docs.type_of_material) %>%
  dplyr::summarize(count=n()) %>%
  mutate(percent = (count / sum(count))*100) %>%
  ggplot() +
  geom_bar(aes(y=percent, x=response.docs.type_of_material, fill=response.docs.type_of_material), stat = "identity") + coord_flip()

```

### Visualize data in News and other

```{r, message=F, warning = F, echo=TRUE, echo=TRUE}
#Make another column of News versus Other ... The labels
allNYTSearch$NewsOrOther = ifelse(allNYTSearch$response.docs.type_of_material == "News","News","Other")
#There is an NA in NewsOrOther
# Visualize coverage of News or Other
allNYTSearch[!is.na(allNYTSearch$NewsOrOther),] %>% 
  group_by(NewsOrOther) %>%
  dplyr::summarize(count=n()) %>%
  mutate(percent = (count / sum(count))*100) %>%
  ggplot() +
  geom_bar(aes(y=percent, x=NewsOrOther, fill=NewsOrOther), stat = "identity") + coord_flip()
```


### Train and Test Split 70%/30%

```{r, message=F, warning = F, echo=TRUE, echo=TRUE}
set.seed(2)
trainInd = sample(seq(1,dim(allNYTSearch)[1],1),round(.5*dim(allNYTSearch)[1]))
allNYTSearchTrain = allNYTSearch[trainInd,]
allNYTSearchTest = allNYTSearch[-trainInd,]

```

### This function returns P(News | Keyword) 
### P(News|KW) = P(KW|News)* P(News) / P(KW)

```{r, message=F, warning = F, echo=TRUE, echo=TRUE}
Pnews_word = function(key_word, trainingSet, alphaLaplace = 1, betaLaplace = 1) # alpha and beta are for laplace smoothing
{
  trainingSet$response.docs.snippet = unlist(str_replace_all(trainingSet$response.docs.snippet,"[^[:alnum:] ]", "")) #Take out all but alpha numeric characters from training headlines
  
  #print(key_word)
  NewsGroup = trainingSet[trainingSet$NewsOrOther == "News",]
  OtherGroup = trainingSet[trainingSet$NewsOrOther == "Other",]
  
  pNews = dim(NewsGroup)[1] / (dim(NewsGroup)[1] + dim(OtherGroup)[1])
  pOther = 1 - pNews
  
  pKWGivenNews = (length(str_which(NewsGroup$response.docs.snippet,regex(str_c("\\b",key_word,"\\b",sep=""),ignore.case = TRUE)))+alphaLaplace)/(dim(NewsGroup)[1]+betaLaplace)
  pKWGivenOther = (length(str_which(OtherGroup$response.docs.snippet,regex(str_c("\\b",key_word,"\\b",sep=""),ignore.case = TRUE)))+alphaLaplace)/(dim(OtherGroup)[1]+betaLaplace)
  
  pKW = length(str_which(trainingSet$response.docs.snippet,regex(str_c("\\b",key_word,"\\b",sep=""),ignore.case = TRUE)))/dim(trainingSet)[1]
  
  pNewsGivenKW = pKWGivenNews*pNews/pKW
  pOtherGivenKW = pKWGivenOther*pOther/pKW
  
  return(pNewsGivenKW)
}
```

### This loop iterates over the articles in the Test Set
```{r, message=F, warning = F, echo=TRUE, echo=TRUE}
theScoreHolderNews = c()
theScoreHolderOther = c()
articleScoreNews = 0;
articleScoreOther = 0;
for (i in 1 : dim(allNYTSearchTest)[1])  #This loop iterates over the articles in the Test Set
{
  
  articleScoreNews = 1; 
  articleScoreOther = 1;
#The [^[:alnum:] ] replaces all non alphanumeric characters with nulls.  
theText = unlist(str_split(str_replace_all(allNYTSearchTest[i,]$response.docs.snippet,"[^[:alnum:] ]", ""), boundary("word"))) #Take out all but alpha numeric characters from search string ... theText holds each word in the headline as its own word.  
# stopwords() #from package tm
wordsToTakeOut = stopwords()
# put word boundaries stopwords so that we don't detect partial words later
wordsToTakeOut = str_c(wordsToTakeOut,collapse = "\\b|\\b") 
wordsToTakeOut = str_c("\\b",wordsToTakeOut,"\\b")
#wordsToTakeOut
importantWords = theText[!str_detect(theText,regex(wordsToTakeOut,ignore_case = TRUE))]
importantWords = theText[!str_detect(theText,regex(wordsToTakeOut,ignore_case = TRUE))]
#importantWords
  for(j in 1 : length(importantWords))  #This loop iterates over the important words in the headline
  {
    articleScoreNews = articleScoreNews * Pnews_word(importantWords[j],allNYTSearchTrain)
    articleScoreOther = articleScoreOther * (1 - Pnews_word(importantWords[j],allNYTSearchTrain))
  }
  theScoreHolderNews[i] = articleScoreNews
  theScoreHolderOther[i] = articleScoreOther
}

```

### Classify the aricle as News or Other based on a given piece of information from the article.

```{r, message=F, warning = F, echo=TRUE, echo=TRUE}
allNYTSearchTest$Classified = ifelse(theScoreHolderNews > theScoreHolderOther,"News","Other")

```

### Confusion Matrix

#### The snippet classifier is better than the headline classifier. The accuracy in the snippet model is 0.6953, the accuracy in the headline clasiffier is 0.542. The snippet classifier has better accuracy. Also the snippet classifier has better detection rate, the detection rate in the snippet classifier is 0.67689 versus 0.3282 in the headline classifier. The sensitivity in the snippet classifier is 0.88740 the sensitivity in the headline classifier is 05059. The snippet classifier has better sensitivity. The specificity in the snippet classifier is  0.07759 the specificity in the headline classifier is 0.6087. The headline classifier has better specificity.
```{r, message=F, warning = F, echo=TRUE, echo=TRUE}

caret::confusionMatrix(table(allNYTSearchTest$Classified,allNYTSearchTest$NewsOrOther))

```

